{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict ED_dispo with result from Lbl2TransformerVec and without"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For architecture and weight initialization, we will compare:\n",
    "\n",
    "• BERT (baseline)\n",
    "\n",
    "• BioBERT\n",
    "\n",
    "• Clinical BioBERT\n",
    "\n",
    "• BlueBERT\n",
    "\n",
    "• XLNet (baseline)\n",
    "\n",
    "• Clinical XLnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-text --no-dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from transformers import TFXLNetModel, XLNetTokenizer\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import tensorflow_hub as hub\n",
    "import json\n",
    "import csv \n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import tensorflow_text as text\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlflow\n",
    "#!databricks configure --host https://community.cloud.databricks.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env  DATABRICKS_USERNAME= 'karengar@stanford.edu' \n",
    "%env  DATABRICKS_PASSWORD = 'Projectcs224*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('/root/data/ed_test.csv')\n",
    "data_train = pd.read_csv('/root/data/ed_train.csv')\n",
    "data_val = pd.read_csv('/root/data/ed_val.csv')\n",
    "data_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.groupby('ED_dispo').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_train.groupby('ED_dispo').size()/data_train.groupby('ED_dispo').size().sum()) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.groupby('ED_dispo').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_test.groupby('ED_dispo').size()/data_test.groupby('ED_dispo').size().sum())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val.groupby('ED_dispo').size().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_val.groupby('ED_dispo').size()/data_val.groupby('ED_dispo').size().sum()) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_l2v = pd.read_csv('/root/Project-CS224N-ED-Disposition/result_l2v_v1.csv') \n",
    "#result_l2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_proba, y_test, labels, mlflow, split_type):\n",
    "    # Plots the Probability Distributions and the ROC Curves One vs Rest\n",
    "    fig = plt.figure(figsize = (17, 8))\n",
    "    bins = [i/20 for i in range(20)] + [1]\n",
    "    roc_auc_ovr = {}\n",
    "    for i in range(len(labels)):\n",
    "        # Gets the class\n",
    "        c = labels[i]\n",
    "\n",
    "        # Prepares an auxiliar dataframe to help with the plots\n",
    "        df_aux = pd.DataFrame()\n",
    "        df_aux['class'] = [1 if y == c else 0 for y in y_test]\n",
    "        df_aux['prob'] = y_proba[:, i]\n",
    "        df_aux = df_aux.reset_index(drop = True)\n",
    "\n",
    "        # Plots the probability distribution for the class and the rest\n",
    "        fig_upper = plt.subplot(2, 4, i+1)\n",
    "        x = df_aux[df_aux['class'] ==0]\n",
    "        plt.hist(x['prob'], density=True, label='Rest')\n",
    "        y = df_aux[df_aux['class'] ==1]\n",
    "        plt.hist(y['prob'], density=True, label=f\" {c}\", bins = bins)\n",
    "        plt.title(c)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.xlabel(f\"P(x = {c})\")\n",
    "        #plt.show()\n",
    "        #mlflow.log_figure(fig, f\"Histogram: {c}.png\") \n",
    "\n",
    "\n",
    "        # Calculates the ROC Coordinates and plots the ROC Curves\n",
    "        fpr, tpr, thresholds = roc_curve(df_aux['class'], df_aux['prob'], pos_label=1)\n",
    "        mauc = auc(fpr, tpr)\n",
    "        fig_bottom = plt.subplot(2, 4, i+5)\n",
    "        plt.title( f\"ROC: {c}\")\n",
    "        plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % mauc)\n",
    "        plt.legend(loc = 'lower right')\n",
    "        plt.plot([0, 1], [0, 1],'r--')\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        #plt.show()\n",
    "        mlflow.log_metric(\"auc_\" + split_type + \"_\"+  c, mauc) \n",
    "\n",
    "    plt.tight_layout()\n",
    "    mlflow.log_figure(fig, \"Hist_and_ROC_\" + c+ \"_\"+ split_type + \".png\") \n",
    "    \n",
    "def evaluate(y_test, y_pred, mlflow,  split_type):\n",
    "    \"\"\"\n",
    "    Evaluation function. For each of the text in evaluation data, it reads the score from\n",
    "    the predictions made. And based on this, it calculates the values of\n",
    "    True positive, True negative, False positive, and False negative.\n",
    "\n",
    "    :param y_test: true labels\n",
    "    :param y_pred: predicted labels\n",
    "    :param labels: list of possible labels\n",
    "    :return: evaluation metrics for classification like, precision, recall, and f_score\n",
    "    \"\"\"\n",
    "     \n",
    "    b = np.zeros_like(y_pred)\n",
    "    b[np.arange(len(y_pred)), y_pred.argmax(1)] = 1\n",
    "    y_proba = y_pred\n",
    "\n",
    "    y_pred = pd.DataFrame(b, columns = list(y_test.columns))\n",
    "    y_pred = y_pred.idxmax(axis=1)\n",
    "    y_test = y_test.idxmax(axis=1)\n",
    "\n",
    "    labels = list(y_test.unique())\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    plot_roc_curve(y_proba, y_test, labels, mlflow, split_type)\n",
    "\n",
    "    \n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # importing accuracy_score, precision_score, recall_score, f1_score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average= None)\n",
    "    recall = recall_score(y_test, y_pred, average= None)\n",
    "    f1score = f1_score(y_test, y_pred, average= None)\n",
    "\n",
    "\n",
    "    report = classification_report(y_test, y_pred, target_names = labels)\n",
    "    print(report)\n",
    "    f_name = 'report_ '+ split_type +'.yaml'\n",
    "    mlflow.log_dict(report, f_name)\n",
    "    #mlflow.log_metric(\"auc_\" + split_type, mauc) \n",
    "    mlflow.log_metric(\"accuracy_\"+ split_type, accuracy)\n",
    "   \n",
    "\n",
    "    return accuracy, precision, recall, f1score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mlflow_log_parameters(parameter):\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"dropout\", parameter['dropout'])\n",
    "    mlflow.log_param(\"learning_rate\", parameter['learning_rate'])\n",
    "    mlflow.log_param(\"epochs\", parameter['epochs'])\n",
    "    mlflow.log_param(\"batch_size\", parameter['batch_size'])\n",
    "\n",
    "\n",
    "def get_inputs(impresions, tokenizer, max_len=120):\n",
    "    \"\"\" Gets tensors from text using the tokenizer provided\"\"\"\n",
    "    inps = [tokenizer.encode_plus(t, max_length=max_len, pad_to_max_length=True, add_special_tokens=True) for t in impresions]\n",
    "    inp_tok = np.array([a['input_ids'] for a in inps])\n",
    "    ids = np.array([a['attention_mask'] for a in inps])\n",
    "    segments = np.array([a['token_type_ids'] for a in inps])\n",
    "    return inp_tok, ids, segments\n",
    "\n",
    "def create_bert(learning_rate=1e-3, dropout=0.1):\n",
    "    # Input layer\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='Text')\n",
    "    text_input.trainable = False\n",
    "    \n",
    "    # Bert encoding\n",
    "    bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\", name = \"Bert_preprocessing\")\n",
    "    # Freeze the bert_preprocess layer\n",
    "    bert_preprocess.trainable = False\n",
    "    \n",
    "    bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/4\", name = \"Bert_encoding\")\n",
    "    # Freeze the bert_encoder layer\n",
    "    bert_encoder.trainable = False\n",
    "    \n",
    "    preprocessed_text = bert_preprocess(text_input)\n",
    "    outputs = bert_encoder(preprocessed_text)\n",
    "    \n",
    "    # Classification layer with dropout for regularization\n",
    "    layer1 = tf.keras.layers.Dropout(dropout, name='Dropout')(outputs['pooled_output'])\n",
    "    # Freeze the layer1\n",
    "    layer1.trainable = False\n",
    "\n",
    "    layer2 = tf.keras.layers.Dense(4, activation='softmax', name='outputs')(layer1)\n",
    "    layer2.trainable = False\n",
    "    # Compile model\n",
    "    model = tf.keras.Model(inputs=[text_input], outputs=[layer2])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='categorical_crossentropy', \n",
    "                   metrics=[tf.keras.metrics.AUC(), 'accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def training_test_mae( mlflow, history):\n",
    "    # Plot training and test loss at each epoch \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(history.history['accuracy'], label='Training acc')\n",
    "    #plt.plot(history.history['val_accuracy'], label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    mlflow.log_figure(fig, \"training_validation_accuracy.png\") \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(history.history['loss'])\n",
    "    #plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    #plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    mlflow.log_figure(fig, \"training_validation_loss.png\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Without using results from l2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(\"/Users/karengar@stanford.edu/BERT_l2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[list(result_l2v.iloc[:, 3:].columns)] = result_l2v.iloc[:, 3:]\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train.drop(['Unnamed: 0', 'ED_dispo', 'CSN', 'Rads_order_ID'], axis=1)\n",
    "X_test =  data_test.drop(['Unnamed: 0', 'ED_dispo', 'CSN', 'Rads_order_ID'], axis=1)\n",
    "X_val = data_val.drop(['Unnamed: 0', 'ED_dispo', 'CSN', 'Rads_order_ID'], axis=1)\n",
    "\n",
    "# Assigning numerical values and storing in another column\n",
    "y_train =  pd.get_dummies(data_train['ED_dispo'])\n",
    "y_test = pd.get_dummies(data_test['ED_dispo'])[list(y_train.columns)]\n",
    "y_val =  pd.get_dummies(data_val['ED_dispo'])[list(y_train.columns)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_train, data_test, data_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "param_dist = {    'dropout': [0.1,0.2, 0.3],\n",
    "                  'learning_rate': loguniform.rvs(1e-3, 1e-1, size= 10),\n",
    "                  'epochs': [1],\n",
    "                  'batch_size':[128]\n",
    "                  }\n",
    "            \n",
    "dict_parameters = ParameterSampler(param_distributions=param_dist, n_iter= 10, random_state=rng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in dict_parameters:\n",
    "    print(parameter)\n",
    "    print('Training')\n",
    "    with mlflow.start_run(): \n",
    "        mlflow_log_parameters(parameter) \n",
    "        bert_model = create_bert(parameter['learning_rate'], parameter['dropout'])\n",
    "        hist = bert_model.fit(x=X_train['Impression'], y=y_train, epochs=parameter['epochs'], batch_size=parameter['batch_size'])\n",
    "        training_test_mae( mlflow, hist)\n",
    "        \n",
    "        print('Testing')\n",
    "        split_type = 'test'\n",
    "        y_pred = bert_model.predict(X_test['Impression'])\n",
    "        accuracy, precision, recall, f1score = evaluate(y_test, y_pred, mlflow, split_type)\n",
    "        \n",
    "        print('Validation')\n",
    "        split_type = 'validation'\n",
    "        y_pred = bert_model.predict(X_val['Impression'])\n",
    "        accuracy, precision, recall, f1score = evaluate(y_val, y_pred, mlflow,  split_type)\n",
    "        \n",
    "        mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
