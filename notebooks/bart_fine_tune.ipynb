{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2605b87a",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- change output to {0: 'contradiction', 1: 'neutral', 2: 'entailment'} format instead of tokenized encoding\n",
    "- how send model/data to gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72d58c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbdf5cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 390 MB.\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "    \n",
    "print_gpu_utilization()\n",
    "\n",
    "import os\n",
    "os.environ['DISABLE_MLFLOW_INTEGRATION'] = 'TRUE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36d756b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 10:01:53.475883: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 10:01:53.593581: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-09 10:01:54.179181: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/lib:/opt/amazon/efa/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/lib:\n",
      "2023-03-09 10:01:54.179284: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/lib:/opt/amazon/efa/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/lib:\n",
      "2023-03-09 10:01:54.179292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 390 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from tqdm.auto import tqdm\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eaaa808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 390 MB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'contradiction': 0, 'entailment': 2, 'neutral': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "print_gpu_utilization()\n",
    "model.config.label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e67abdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-50c8ec55ede3b71e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523a344a29554e2db63deaca3c413b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 390 MB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'Report Impression', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices', 'No Finding'],\n",
       "        num_rows: 102304\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['Unnamed: 0', 'Report Impression', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices', 'No Finding'],\n",
       "        num_rows: 29230\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'Report Impression', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices', 'No Finding'],\n",
       "        num_rows: 14615\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files={\n",
    "    \"train\": \"/root/data/chex_train.csv\",\n",
    "    \"val\": \"/root/data/chex_val.csv\",\n",
    "    \"test\": \"/root/data/chex_test.csv\",\n",
    "})\n",
    "print_gpu_utilization()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ff93284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 8231,\n",
       " 'Report Impression': \"1.  Extensive cecal wall thickening and inflammatory changes with suspected pneumatosis and evidence of extraluminal mesenteric gas, and trace portal venous gas, in keeping with bowel ischemia. No frank disruption in the bowel contour is seen on noncontrast images. No abscess or drainable fluid collection. 2.  Normal short appendix. 3.  Moderate-sized bilateral pleural effusions with a partially visualized nodular opacity in the right middle lobe, likely representing focal atelectasis. Other less likely etiologies include consolidation or pulmonary nodule, and when the patient's status improves, further assessment with CT chest could be considered. 4.  Compression fracture of L1 with bony retropulsion. This is new from the radiographs of 2/3/2019, but still appears chronic. Correlation with point tenderness recommended. Dr. Li discussed these findings with Dr. Cohen via telephone on 9/19/2020 at 4:10 AM..\",\n",
       " 'Enlarged Cardiomediastinum': None,\n",
       " 'Cardiomegaly': None,\n",
       " 'Lung Opacity': 1.0,\n",
       " 'Lung Lesion': 1.0,\n",
       " 'Edema': None,\n",
       " 'Consolidation': -1.0,\n",
       " 'Pneumonia': None,\n",
       " 'Atelectasis': 1.0,\n",
       " 'Pneumothorax': None,\n",
       " 'Pleural Effusion': 1.0,\n",
       " 'Pleural Other': None,\n",
       " 'Fracture': 1.0,\n",
       " 'Support Devices': None,\n",
       " 'No Finding': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][3] # contains Atelectasis, Pleural Effusion, and Fracture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bf7d07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][3]['Fracture'] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "925d86d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-50c8ec55ede3b71e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-547ad8efa1a8a10c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-50c8ec55ede3b71e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-635749ddea230bea.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-50c8ec55ede3b71e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d7e2dcabf2220118.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Report Impression', 'target', 'labels'],\n",
       "    num_rows: 716128\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [\"Fracture\", \"Edema\", \"Cardiomegaly\", \"Pneumonia\", \"Atelectasis\", \"Pneumothorax\", \"Pleural Effusion\"]\n",
    "\n",
    "# function(batch: Dict[str, List]) -> Dict[str, List]\n",
    "def create_target_sentences(batch):\n",
    "    text_key = 'Report Impression'\n",
    "    out = {'target': [], text_key: [], 'labels': []}\n",
    "    for i in range(len(batch[text_key])):\n",
    "        for label in labels:\n",
    "            out['target'].append(f'This example is {label}.')\n",
    "            out[text_key].append(batch[text_key][i])\n",
    "            if batch[label][i] == -1:\n",
    "                out['labels'].append(model.config.label2id['contradiction'])\n",
    "            elif batch[label][i] == None or batch[label][i] == 0:\n",
    "                out['labels'].append(model.config.label2id['neutral'])\n",
    "            elif batch[label][i] == 1:\n",
    "                out['labels'].append(model.config.label2id['entailment'])\n",
    "            else:\n",
    "                raise Exception(f\"invalid value in labels {batch[label][i]}\")\n",
    "    return out\n",
    "    \n",
    "dataset_with_labels = dataset.map(\n",
    "    create_target_sentences,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    ")\n",
    "dataset_with_labels['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1100b8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset_with_labels['train']['labels'][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d04eeaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Report Impression', 'target']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_columns = dataset_with_labels['train'].column_names\n",
    "remove_columns.remove('labels') # keep the labels column!\n",
    "remove_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7447227e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-50c8ec55ede3b71e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9e75d602e447abcd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-50c8ec55ede3b71e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b5380fdb8f3ab1e2.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-50c8ec55ede3b71e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-18795d90473e17a5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-50c8ec55ede3b71e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8ed8aaec53942f0a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 390 MB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: max_length may be slow?\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(text=examples[\"Report Impression\"], text_pair=examples[\"target\"], padding=\"max_length\", truncation='only_first')\n",
    "\n",
    "# tokenized_datasets = dataset_with_labels.map(\n",
    "#     tokenize_function,\n",
    "#     batched=True,\n",
    "#     remove_columns=remove_columns,\n",
    "# )\n",
    "\n",
    "# small_train_dataset = dataset_with_labels['train'].shuffle(seed=42).select(range(5)).map(\n",
    "small_train_dataset = dataset_with_labels['train'].shuffle(seed=42).select(range(1000)).map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=remove_columns,\n",
    ")\n",
    "\n",
    "# small_val_dataset = dataset_with_labels['val'].shuffle(seed=42).select(range(4)).map(\n",
    "small_val_dataset = dataset_with_labels['val'].shuffle(seed=42).select(range(200)).map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=remove_columns,\n",
    ")\n",
    "\n",
    "print_gpu_utilization()\n",
    "small_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c54cf4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79da41bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartTokenizerFast(name_or_path='facebook/bart-large-mnli', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6311ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(10))\n",
    "# small_val_dataset = tokenized_datasets['val'].shuffle(seed=42).select(range(10))\n",
    "# print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "195a4255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 393 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits_tuple, labels = eval_pred\n",
    "#     print(labels.shape, labels)\n",
    "#     print(type(logits_tuple), logits_tuple[0].shape, logits_tuple[1].shape)\n",
    "#     print(logits_tuple)\n",
    "    logits, _ = logits_tuple\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "#     print(predictions.shape)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4529ee79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 2654 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"test_trainer_bart\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "# trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d0e79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/l2v/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3000\n",
      "  Number of trainable parameters = 407344131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 2654 MB.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1001' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1001/3000 17:09 < 34:19, 0.97 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 57/200 00:19 < 00:48, 2.94 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test_trainer_bart/checkpoint-500\n",
      "Configuration saved in test_trainer_bart/checkpoint-500/config.json\n",
      "Model weights saved in test_trainer_bart/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to test_trainer_bart/checkpoint-1000\n",
      "Configuration saved in test_trainer_bart/checkpoint-1000/config.json\n",
      "Model weights saved in test_trainer_bart/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 1\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483dc609",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
    "logits, predicted_class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f5ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb34281",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config._num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8287669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does model need to have problem_type=\"multi_label_classification\"?\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(model.config.id2label)\n",
    "\n",
    "labels = torch.sum(\n",
    "    torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n",
    ").to(torch.float)\n",
    "loss = model(**inputs, labels=labels).loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3191c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea2e2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://huggingface.co/joeddav/bart-large-mnli-yahoo-answers\n",
    "label = 'cat'\n",
    "premise = 'I love cats and dogs'\n",
    "hypothesis = f'This example is {label}.'\n",
    "\n",
    "# run through model pre-trained on MNLI\n",
    "device = 0\n",
    "x = tokenizer.encode(premise, hypothesis, return_tensors='pt', truncation_strategy='only_first')\n",
    "print_gpu_utilization()\n",
    "logits = model(x.to(device))[0]\n",
    "print(logits)\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d15f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tokenizer.encode(premise, return_tensors='pt', truncation_strategy='only_first')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ea18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss=None????\n",
    "out = model(x.to(device))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d83527",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f598809",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29608825",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe0cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f24d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "entail_contradiction_logits = logits[:,[0,2]]\n",
    "probs = entail_contradiction_logits.softmax(dim=1)\n",
    "prob_label_is_true = probs[:,1]\n",
    "prob_label_is_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434584f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(premise, premise, premise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f6a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f6f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
