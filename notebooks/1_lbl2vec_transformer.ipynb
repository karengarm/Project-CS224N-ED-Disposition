{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chexbert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Chexbert like a groudntruth for 14 medical observations: <b>Fracture</b> , Consolidation, Enlarged Cardiomediastinum, No Finding, Pleural Other, <b>Cardiomegaly</b>, <b>Pneumothorax</b>, <b>Atelectasis</b>, Support Devices, <b>Edema</b>, <b>Pleural Effusion</b>, Lung Lesion, Lung Opacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this group 6  medical observations are relevant:\n",
    "\n",
    "Fracture:  Rib fracture, Skull fracture, Rib fracture\n",
    "\n",
    "Cardiomegaly\n",
    "\n",
    "Pneumothorax\n",
    "\n",
    "Atelectasis\n",
    "\n",
    "Edema: Pulmonary edema, Cerebral edema\n",
    "\n",
    "Pleural Effusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('~/rads_dispo_lim_2023_02_23.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /root/Project-CS224N-ED-Disposition/CheXbert-Labeler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run using conda activate chexbert11 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python src/label.py -d=/root/impressions.csv -o=/root/Project-CS224N-ED-Disposition/CheXbert-Labeler -c=/root/chexbert.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_chexbert = pd.read_csv('/root/Project-CS224N-ED-Disposition/CheXbert-Labeler/labeled_reports_output.csv')\n",
    "result_chexbert.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lbl2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run using conda activate l2v "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With labl2Vec we can predict medical observations using the column Impression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "from lbl2vec import Lbl2TransformerVec\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /root/Project-CS224N-ED-Disposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "\t[\"Pneumonia\"],\n",
    "\t[\"Pneumothorax\"],\n",
    "\t[\"Pleural Effusion\"],\n",
    "    ['Edema'],\n",
    "    ['Fracture'],\n",
    "#\t[\"Pulmonary edema\"],\n",
    "#\t[\"Rib fracture\"],\n",
    "\t[\"Infection\"],\n",
    "\t[\"Aspiration\"],\n",
    "\t[\"Cardiomegaly\"],\n",
    "\t[\"Opacities\"],\n",
    "\t[\"Atelectasis\"],\n",
    "\t[\"Intracranial hemorrhage\"],\n",
    "\t[\"Subarachnoid hemorrhage\"],\n",
    "\t[\"Subdural hemorrhage\"],\n",
    "\t[\"Epidural hemorrhage\"],\n",
    "\t[\"Intraparenchymal hemorrhage\"],\n",
    "\t[\"Intraventricular hemorrhage\"],\n",
    "#\t[\"Skull fracture\"],\n",
    "\t[\"Stroke\"],\n",
    "#\t[\"Cerebral edema\"],\n",
    "\t[\"Diffuse axonal injury\"],\n",
    "\t[\"Appendicitis\"],\n",
    "\t[\"Cholecystitis\"],\n",
    "\t[\"Abdominal Aortic Aneurysm\"],\n",
    "\t[\"Small bowel obstruction\"],\n",
    "\t[\"Pancreatitis\"],\n",
    "\t[\"Splenic laceration\"],\n",
    "\t[\"Liver laceration\"],\n",
    "\t[\"Colitis\"],\n",
    "\t[\"Pyelonephritis\"],\n",
    "\t[\"Nephrolithiasis\"],\n",
    "\t[\"Malignancy\"],\n",
    "\t[\"Pericaridial effusion\"],\n",
    "\t[\"Aortic dissection\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run  Lbl2TransformerVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model using the default transformer-embedding model (\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "label_name = [i[0] for i in keywords]\n",
    "\n",
    "model = Lbl2TransformerVec(\n",
    "    keywords_list=keywords,\n",
    "    documents=data[\"Impression\"],\n",
    "    label_names = label_name \n",
    ")\n",
    "model.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get similarity scores from trained model\n",
    "result_l2v = model.predict_model_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save csv \n",
    "result_l2v.to_csv('/root/Project-CS224N-ED-Disposition/result_l2v_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_l2v.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare results from Chexbert and results from Lbl2TransformerVec on common keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparing the label with the higher score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mapping between keywords and labels from chexbert and l2v. They have 7 keywords in common:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fracture:  Rib fracture, Skull fracture\n",
    "\n",
    "Cardiomegaly\n",
    "\n",
    "Pneumothorax\n",
    "\n",
    "Atelectasis\n",
    "\n",
    "Edema: Pulmonary edema, Cerebral edema\n",
    "\n",
    "Pleural Effusion\n",
    "\n",
    "Pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib\n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Result l2v</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_l2v = pd.read_csv('/root/Project-CS224N-ED-Disposition/result_l2v_v1.csv') \n",
    "result_l2v.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select second highest values\n",
    "list_labels = list(result_l2v.columns)\n",
    "list_labels.remove(\"doc_key\")\n",
    "list_labels.remove(\"most_similar_label\")\n",
    "list_labels.remove(\"highest_similarity_score\")\n",
    "result_l2v['second_similar_label'] = result_l2v[list_labels].columns[np.argpartition(result_l2v[list_labels].values, -2)[:,-2]]\n",
    "result_l2v[['most_similar_label','second_similar_label']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_keywords_l2v = ['Fracture', 'Cardiomegaly', 'Pneumothorax', 'Atelectasis',\n",
    "                          'Edema', 'Pleural Effusion', 'Pneumonia', 'No Finding']\n",
    "\n",
    "# Substitue values from  most_similar_label that are not in important_keywords_l2v as 'No Finding'\n",
    "list_nofinding = list(set(list(result_l2v.columns)) - set(important_keywords_l2v )) \n",
    "result_l2v = result_l2v.replace(list_nofinding, [np.nan]*len(list_nofinding))\n",
    "result_l2v[['most_similar_label', 'second_similar_label']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine most_similar_label with second_similar_label\n",
    "#replace null values with values from another column in dataframe?\n",
    "result_l2v['combine'] = (result_l2v['most_similar_label'].fillna(result_l2v['second_similar_label'])\n",
    "                     .fillna('No Finding')\n",
    "                 )\n",
    "\n",
    "result_l2v['most_similar_label'] = result_l2v[['most_similar_label']].fillna('No Finding')\n",
    "result_l2v[['most_similar_label', 'second_similar_label', 'combine']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_keywords_l2v = ['Fracture', 'Cardiomegaly', 'Pneumothorax', 'Atelectasis',\n",
    "                          'Edema', 'Pleural Effusion', 'Pneumonia']\n",
    "result_l2v_multiple = result_l2v[important_keywords_l2v]\n",
    "\n",
    "# Replace maximum value of each row for one\n",
    "result_l2v_multiple.values[range(len(result_l2v_multiple.index)), np.argmax(result_l2v_multiple.values, axis=1)] = 1\n",
    "result_l2v_multiple[result_l2v_multiple < 1] = 0\n",
    "result_l2v_multiple.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_l2v_multiple_2 = result_l2v[important_keywords_l2v]\n",
    "\n",
    "# Replace maximum and second value of each row for one\n",
    "result_l2v_multiple_2.values[range(len(result_l2v_multiple_2.index)), np.argmax(result_l2v_multiple_2.values, axis=1)] = 1\n",
    "# Replace second maximum value of each row for one\n",
    "result_l2v_multiple_2.values[range(len(result_l2v_multiple_2.index)), np.argpartition(result_l2v_multiple_2.values, -2)[:,-2]] = 1\n",
    "\n",
    "result_l2v_multiple_2[result_l2v_multiple_2 < 1] = 0\n",
    "result_l2v_multiple_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Result chexbert</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_chexbert = pd.read_csv('/root/Project-CS224N-ED-Disposition/CheXbert-Labeler/labeled_reports_output.csv')\n",
    "result_chexbert.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_keywords_chexbert = ['Fracture', 'Cardiomegaly', 'Pneumothorax', \n",
    "                               'Atelectasis','Edema', 'Pleural Effusion', 'Pneumonia', 'No Finding'] \n",
    "#result_chexbert.replace(0, np.nan, inplace=True)\n",
    "result_chexbert.replace(-1, 0, inplace=True)\n",
    "result_chexbert.replace(np.nan, 0, inplace=True)\n",
    "\n",
    "result_chexbert['most_similar_label'] = result_chexbert[important_keywords_chexbert].idxmax(1)\n",
    "result_chexbert['most_similar_label']  = result_chexbert['most_similar_label'] .fillna('No Finding')\n",
    "# Select columns from result_chexbert included in important_keywords\n",
    "#important_keywords_chexbert.append('Report Impression')\n",
    "result_chexbert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_keywords_chexbert = ['Fracture', 'Cardiomegaly', 'Pneumothorax', \n",
    "                               'Atelectasis','Edema', 'Pleural Effusion', 'Pneumonia']\n",
    "result_chexbert_multiple = result_chexbert[important_keywords_chexbert]\n",
    "result_chexbert_multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluation function. For each of the text in evaluation data, it reads the score from\n",
    "    the predictions made. And based on this, it calculates the values of\n",
    "    True positive, True negative, False positive, and False negative.\n",
    "\n",
    "    :param y_test: true labels\n",
    "    :param y_pred: predicted labels\n",
    "    :param labels: list of possible labels\n",
    "    :return: evaluation metrics for classification like, precision, recall, and f_score\n",
    "    \"\"\"\n",
    "    y_pred = list(y_pred)\n",
    "    y_test = list(y_test)\n",
    "    \n",
    "    labels = list(set(y_test  + y_pred))\n",
    "    labels = sorted(labels)\n",
    "    \n",
    "    confusion = confusion_matrix(y_test, y_pred, labels= labels)\n",
    "    print('Confusion Matrix\\n')\n",
    "    print(confusion)\n",
    "\n",
    "    df_cm = pd.DataFrame(confusion, index=[i for i in labels],\n",
    "                         columns=[i for i in labels])\n",
    "\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "\n",
    "    # importing accuracy_score, precision_score, recall_score, f1_score\n",
    "    Accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('\\nAccuracy: {:.2f}\\n'.format(Accuracy))\n",
    "\n",
    "    Micro_Precision = precision_score(y_test, y_pred, average='micro')\n",
    "    print('Micro Precision: {:.2f}'.format(Micro_Precision))\n",
    "\n",
    "    Micro_Recall = recall_score(y_test, y_pred, average='micro')\n",
    "    print('Micro Recall: {:.2f}'.format(Micro_Recall))\n",
    "\n",
    "    Micro_F1score = f1_score(y_test, y_pred, average='micro')\n",
    "    print('Micro F1-score: {:.2f}\\n'.format(Micro_F1score))\n",
    "\n",
    "    Macro_Precision = precision_score(y_test, y_pred, average='macro')\n",
    "    print('Macro Precision: {:.2f}'.format(Macro_Precision))\n",
    "\n",
    "    Macro_Recall = recall_score(y_test, y_pred, average='macro')\n",
    "    print('Macro Recall: {:.2f}'.format(Macro_Recall))\n",
    "\n",
    "    Macro_F1score = f1_score(y_test, y_pred, average='macro')\n",
    "    print('Macro F1-score: {:.2f}\\n'.format(Macro_F1score))\n",
    "\n",
    "    Weighted_Precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    print('Weighted Precision: {:.2f}'.format(Weighted_Precision))\n",
    "\n",
    "    Weighted_Recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    print('Weighted Recall: {:.2f}'.format(Weighted_Recall))\n",
    "\n",
    "    Weighted_F1score = f1_score(y_test, y_pred, average='weighted')\n",
    "    print('Weighted F1-score: {:.2f}'.format(Weighted_F1score))\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    print('\\nClassification Report\\n')\n",
    "    report = classification_report(y_test, y_pred, target_names=labels)\n",
    "    print(report)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- most_similar_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_chexbert['most_similar_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_l2v['most_similar_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = result_chexbert['most_similar_label']\n",
    "y_pred = result_l2v['most_similar_label']\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining for l2v most_similar_label and  and second_similar_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = result_chexbert['most_similar_label']\n",
    "y_pred = result_l2v['combine']\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Label Classification Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mmuratarat.github.io/2020-01-25/multilabel_classification_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "def multi_label_evaluation(y_true, y_pred):\n",
    "    print('Exact Match Ratio: {0}'.format(sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)))\n",
    "\n",
    "    print('Hamming loss: {0}'.format(sklearn.metrics.hamming_loss(y_true, y_pred))) \n",
    "\n",
    "    #\"samples\" applies only to multilabel problems. It does not calculate a per-class measure, instead calculating the metric over the true and predicted classes \n",
    "    #for each sample in the evaluation data, and returning their (sample_weight-weighted) average.\n",
    "\n",
    "    print('Recall: {0}'.format(sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, average='samples'))) \n",
    "\n",
    "\n",
    "    print('Precision: {0}'.format(sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, average='samples')))\n",
    "\n",
    "\n",
    "    print('F1 Measure: {0}'.format(sklearn.metrics.f1_score(y_true=y_true, y_pred=y_pred, average='samples'))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(result_chexbert_multiple)\n",
    "y_pred = np.array(result_l2v_multiple)\n",
    "#y_true = y_true  + y_pred\n",
    "#substitute 2 for 1 and the rest for 0\n",
    "#y_true[y_true != 2] = 0\n",
    "#y_true[y_true == 2] = 1\n",
    "multi_label_evaluation(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(result_chexbert_multiple)\n",
    "y_pred = np.array(result_l2v_multiple_2)\n",
    "y_true = y_true + y_pred\n",
    "#substitute 2 for 1 and the rest for 0\n",
    "y_true[y_true != 2] = 0\n",
    "y_true[y_true == 2] = 1\n",
    "multi_label_evaluation(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
